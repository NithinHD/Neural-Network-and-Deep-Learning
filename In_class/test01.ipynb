{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a scenario where you're tasked with predicting housing prices using linear regression. You've implemented a gradient descent-based approach for training your model. Reflect on the impact of various hyperparameters, such as learning rate and maximum iterations, on the training process and the quality of predictions. Discuss how you would experiment with these hyperparameters to optimize your model's performance, considering factors like convergence speed and prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 09:48:02.394121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.W1 = tf.Variable(tf.random.normal([hidden_size, input_size], stddev=0.01))\n",
    "        self.b1 = tf.Variable(tf.zeros([hidden_size, 1]))\n",
    "        self.W2 = tf.Variable(tf.random.normal([output_size, hidden_size], stddev=0.01))\n",
    "        self.b2 = tf.Variable(tf.zeros([output_size, 1]))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        # Forward pass\n",
    "        Z1 = tf.matmul(self.W1, X) + self.b1\n",
    "        A1 = tf.nn.tanh(Z1)\n",
    "        Z2 = tf.matmul(self.W2, A1) + self.b2\n",
    "        A2 = tf.nn.sigmoid(Z2)\n",
    "        return A2\n",
    "\n",
    "    def backward_propagation(self, X, Y, learning_rate):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.forward_propagation(X)\n",
    "            loss = self.cross_entropy_loss(Y, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, [self.W1, self.b1, self.W2, self.b2])\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "        optimizer.apply_gradients(zip(gradients, [self.W1, self.b1, self.W2, self.b2]))\n",
    "\n",
    "    def train(self, X, Y, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.backward_propagation(X, Y, learning_rate)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                predictions = self.forward_propagation(X)\n",
    "                loss = self.cross_entropy_loss(Y, predictions)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def cross_entropy_loss(self, Y, A):\n",
    "        m = Y.shape[1]\n",
    "        loss = -tf.reduce_mean(Y * tf.math.log(A) + (1 - Y) * tf.math.log(1 - A))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=NeuralNetwork(2, 2, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
